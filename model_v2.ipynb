{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from fairseq.models.roberta import RobertaModel, RobertaHubInterface\n",
    "# from fairseq import hub_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"roberta_large_fairseq\"\n",
    "# loaded = hub_utils.from_pretrained(\n",
    "#     model_name_or_path=model_path,\n",
    "#     data_name_or_path=model_path,\n",
    "#     bpe=\"sentencepiece\",\n",
    "#     sentencepiece_vocab=os.path.join(model_path, \"sentencepiece.bpe.model\"),\n",
    "#     load_checkpoint_heads=True,\n",
    "#     archive_map=RobertaModel.hub_models(),\n",
    "#     cpu=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(output[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern =  '@[^\\s]+'\n",
    "def clean_text(pattern, text, tag):\n",
    "    \n",
    "    rgx_list = re.findall(pattern, text)\n",
    "    \n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, tag, new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def read_data(test_path):\n",
    "    test = dict()\n",
    "\n",
    "    test['tags'] = pd.read_csv(os.path.join(test_path, 'test_set_only_tags.txt'), header=None)\n",
    "    test['tags'].columns = ['label']\n",
    "\n",
    "    test['text'] = pd.read_table(os.path.join(test_path, 'test_set_only_text.txt'), header=None)\n",
    "    test['text'].columns = ['text']\n",
    "\n",
    "\n",
    "    test['tags']['label_text'] = test['tags']['label'].map({0: \"non-harmful\",\n",
    "                                                              1: \"cyberbullying\",\n",
    "                                                              2: \"hate-speech\"})\n",
    "\n",
    "    df = pd.concat([test['tags'], test['text']], axis=1)\n",
    "    df['text'] = df['text'].apply(lambda x: clean_text(pattern, x, '<TWITTER_HANDLE>'))\n",
    "\n",
    "    X_test = df['text']\n",
    "    y_test = df['label']\n",
    "    \n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = read_data('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for sentence in X_test[:100]:\n",
    "    input = tokenizer.encode(sentence)\n",
    "    output = model(torch.tensor([input.ids]))[0]\n",
    "    print(len(sentence.split(\" \")), output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '[CLS] Co tam siƒô sta≈Ço [SEP]'\n",
    "input = tokenizer.encode(sentence)\n",
    "output = model(torch.tensor([input.ids]))[0]\n",
    "print(len(sentence.split(\" \")), output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for sentence in X_test:\n",
    "#     input = tokenizer.encode(sentence)\n",
    "#     output = model(torch.tensor([input.ids]))[0]\n",
    "\n",
    "# #X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for sentence in X_test[:100]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sacremoses import MosesDetokenizer\n",
    "\n",
    "emoji = {\n",
    "    'üòÄ': ':D',\n",
    "    'üòÉ': ':)',\n",
    "    'üòÑ': ':)',\n",
    "    'üòÅ': ':)',\n",
    "    'üòÜ': 'xD',\n",
    "    'üòÖ': ':)',\n",
    "    'ü§£': 'xD',\n",
    "    'üòÇ': 'xD',\n",
    "    'üôÇ': ':)',\n",
    "    'üôÉ': ':)',\n",
    "    'üòâ': ';)',\n",
    "    'üòä': ':)',\n",
    "    'üòá': ':)',\n",
    "    'ü•∞': ':*',\n",
    "    'üòç': ':*',\n",
    "    'ü§©': ':*',\n",
    "    'üòò': ':*',\n",
    "    'üòó': ':*',\n",
    "    '‚ò∫': ':)',\n",
    "    'üòö': ':*',\n",
    "    'üòã': ':P',\n",
    "    'üòõ': ':P',\n",
    "    'üòú': ':P',\n",
    "    'üòù': ':P',\n",
    "    'ü§ë': ':P',\n",
    "    'ü§™': ':P',\n",
    "    'ü§ó': ':P',\n",
    "    'ü§≠': ':P',\n",
    "    'ü§´': ':|',\n",
    "    'ü§î': ':|',\n",
    "    'ü§®': ':|',\n",
    "    'üòê': ':|',\n",
    "    'üòë': ':|',\n",
    "    'üò∂': ':|',\n",
    "    'üòè': ':)',\n",
    "    'üòí': ':(',\n",
    "    'üôÑ': ':|',\n",
    "    'ü§ê': ':|',\n",
    "    'üò¨': ':$',\n",
    "    'üòå': 'zzz',\n",
    "    'üòî': ':(',\n",
    "    'üò™': 'zzz',\n",
    "    'ü§§': ':(',\n",
    "    'ü§í': ':(',\n",
    "    'ü§ï': ':(',\n",
    "    'ü§¢': ':(',\n",
    "    'ü§Æ': ':(',\n",
    "    'ü§ß': ':(',\n",
    "    'ü•µ': ':(',\n",
    "    'ü•∂': ':(',\n",
    "    'ü•¥': ':(',\n",
    "    'üòµ': ':(',\n",
    "    'ü§Ø': ':(',\n",
    "    'ü§†': ':)',\n",
    "    'ü•≥': ':)',\n",
    "    'üòé': ':)',\n",
    "    'ü§ì': ':)',\n",
    "    'üßê': ':)',\n",
    "    'üòï': ':(',\n",
    "    'üòü': ':(',\n",
    "    'üôÅ': ':(',\n",
    "    '‚òπ': ':(',\n",
    "    'üòÆ': ':O',\n",
    "    'üòØ': ':O',\n",
    "    'üò≤': ':O',\n",
    "    'üò≥': ':(',\n",
    "    'ü•∫': ':(',\n",
    "    'üò¶': ':(',\n",
    "    'üòß': ':(',\n",
    "    'üò®': ':(',\n",
    "    'üò∞': ':(',\n",
    "    'üò•': ':(',\n",
    "    'üò¢': ':(',\n",
    "    'üò≠': ':(',\n",
    "    'üò±': ':(',\n",
    "    'üòñ': ':(',\n",
    "    'üò£': ':(',\n",
    "    'üòû': ':(',\n",
    "    'üòì': ':(',\n",
    "    'üò©': ':(',\n",
    "    'üò´': ':(',\n",
    "    'ü•±': 'zzz',\n",
    "    'üò§': ':(',\n",
    "    'üò°': ':(',\n",
    "    'üò†': ':(',\n",
    "    'ü§¨': ':(',\n",
    "    'üòà': ']:->',\n",
    "    'üëø': ']:->',\n",
    "    'üíÄ': ':(',\n",
    "    '‚ò†': ':(',\n",
    "    'üíã': ':*',\n",
    "    'üíî': ':(',\n",
    "    'üí§': 'zzz'\n",
    "}\n",
    "\n",
    "class TextNormalizer(object):\n",
    "\n",
    "    def __init__(self, detokenize: bool=True, replace_emoji: bool=True):\n",
    "        self._moses = MosesDetokenizer(lang=\"pl\")\n",
    "        self._detokenize = detokenize\n",
    "        self._replace_emoji = replace_emoji\n",
    "\n",
    "    def process(self, text: str) -> str:\n",
    "        if self._replace_emoji:\n",
    "            text = \"\".join((emoji.get(c, c) for c in text))\n",
    "        if self._detokenize:\n",
    "            text = text.replace(\" em \", \"em \").replace(\" ≈õmy \", \"≈õmy \").replace(\" m \", \"m \")\n",
    "            tokens: List[str] = text.split()\n",
    "            text = self._moses.detokenize(tokens)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Iterable, Callable, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExample(object):\n",
    "\n",
    "    def __init__(self, inputs: Union[str, List], label: str):\n",
    "        self.inputs: List[str] = [inputs] if isinstance(inputs, str) else inputs\n",
    "        self.label: str = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = os.path.join(test_path, 'test_set_only_tags.txt')\n",
    "tags_path = os.path.join(test_path, 'test_set_only_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(self) -> TextNormalizer:\n",
    "        return TextNormalizer(detokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "with open(text_path, \"r\", encoding=\"utf-8\") as text_file, open(tags_path, \"r\", encoding=\"utf-8\") as tags_file:\n",
    "    text_lines = text_file.readlines()\n",
    "    tags_lines = tags_file.readlines()\n",
    "    assert len(text_lines) == len(tags_lines)\n",
    "    for idx in range(len(text_lines)):\n",
    "        text = normalizer.process(text_lines[idx].strip())\n",
    "        text = text.replace(\"@anonymized_account\", \"@ u≈ºytkownik\")\n",
    "        label = tags_lines[idx].strip()\n",
    "        #yield DataExample(text, label)\n",
    "        texts.append(text)\n",
    "        labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchnlp.word_to_vector.FastText(language='en', aligned=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.datasets import imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.word_to_vector import FastText\n",
    "vectors = FastText('pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors['playful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = FastText('pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for sentence in X_test:\n",
    "#     input = tokenizer.encode(sentence)\n",
    "#     output = model(torch.tensor([input.ids]))[0]\n",
    "\n",
    "# #X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
