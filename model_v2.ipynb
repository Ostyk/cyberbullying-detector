{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from fairseq.models.roberta import RobertaModel, RobertaHubInterface\n",
    "# from fairseq import hub_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = \"roberta_large_fairseq\"\n",
    "# loaded = hub_utils.from_pretrained(\n",
    "#     model_name_or_path=model_path,\n",
    "#     data_name_or_path=model_path,\n",
    "#     bpe=\"sentencepiece\",\n",
    "#     sentencepiece_vocab=os.path.join(model_path, \"sentencepiece.bpe.model\"),\n",
    "#     load_checkpoint_heads=True,\n",
    "#     archive_map=RobertaModel.hub_models(),\n",
    "#     cpu=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(output[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern =  '@[^\\s]+'\n",
    "def clean_text(pattern, text, tag):\n",
    "    \n",
    "    rgx_list = re.findall(pattern, text)\n",
    "    \n",
    "    new_text = text\n",
    "    for rgx_match in rgx_list:\n",
    "        new_text = re.sub(rgx_match, tag, new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def read_data(test_path):\n",
    "    test = dict()\n",
    "\n",
    "    test['tags'] = pd.read_csv(os.path.join(test_path, 'test_set_only_tags.txt'), header=None)\n",
    "    test['tags'].columns = ['label']\n",
    "\n",
    "    test['text'] = pd.read_table(os.path.join(test_path, 'test_set_only_text.txt'), header=None)\n",
    "    test['text'].columns = ['text']\n",
    "\n",
    "\n",
    "    test['tags']['label_text'] = test['tags']['label'].map({0: \"non-harmful\",\n",
    "                                                              1: \"cyberbullying\",\n",
    "                                                              2: \"hate-speech\"})\n",
    "\n",
    "    df = pd.concat([test['tags'], test['text']], axis=1)\n",
    "    df['text'] = df['text'].apply(lambda x: clean_text(pattern, x, '<TWITTER_HANDLE>'))\n",
    "\n",
    "    X_test = df['text']\n",
    "    y_test = df['label']\n",
    "    \n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = read_data('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for sentence in X_test[:100]:\n",
    "    input = tokenizer.encode(sentence)\n",
    "    output = model(torch.tensor([input.ids]))[0]\n",
    "    print(len(sentence.split(\" \")), output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '[CLS] Co tam się stało [SEP]'\n",
    "input = tokenizer.encode(sentence)\n",
    "output = model(torch.tensor([input.ids]))[0]\n",
    "print(len(sentence.split(\" \")), output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for sentence in X_test:\n",
    "#     input = tokenizer.encode(sentence)\n",
    "#     output = model(torch.tensor([input.ids]))[0]\n",
    "\n",
    "# #X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for sentence in X_test[:100]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from sacremoses import MosesDetokenizer\n",
    "\n",
    "emoji = {\n",
    "    '😀': ':D',\n",
    "    '😃': ':)',\n",
    "    '😄': ':)',\n",
    "    '😁': ':)',\n",
    "    '😆': 'xD',\n",
    "    '😅': ':)',\n",
    "    '🤣': 'xD',\n",
    "    '😂': 'xD',\n",
    "    '🙂': ':)',\n",
    "    '🙃': ':)',\n",
    "    '😉': ';)',\n",
    "    '😊': ':)',\n",
    "    '😇': ':)',\n",
    "    '🥰': ':*',\n",
    "    '😍': ':*',\n",
    "    '🤩': ':*',\n",
    "    '😘': ':*',\n",
    "    '😗': ':*',\n",
    "    '☺': ':)',\n",
    "    '😚': ':*',\n",
    "    '😋': ':P',\n",
    "    '😛': ':P',\n",
    "    '😜': ':P',\n",
    "    '😝': ':P',\n",
    "    '🤑': ':P',\n",
    "    '🤪': ':P',\n",
    "    '🤗': ':P',\n",
    "    '🤭': ':P',\n",
    "    '🤫': ':|',\n",
    "    '🤔': ':|',\n",
    "    '🤨': ':|',\n",
    "    '😐': ':|',\n",
    "    '😑': ':|',\n",
    "    '😶': ':|',\n",
    "    '😏': ':)',\n",
    "    '😒': ':(',\n",
    "    '🙄': ':|',\n",
    "    '🤐': ':|',\n",
    "    '😬': ':$',\n",
    "    '😌': 'zzz',\n",
    "    '😔': ':(',\n",
    "    '😪': 'zzz',\n",
    "    '🤤': ':(',\n",
    "    '🤒': ':(',\n",
    "    '🤕': ':(',\n",
    "    '🤢': ':(',\n",
    "    '🤮': ':(',\n",
    "    '🤧': ':(',\n",
    "    '🥵': ':(',\n",
    "    '🥶': ':(',\n",
    "    '🥴': ':(',\n",
    "    '😵': ':(',\n",
    "    '🤯': ':(',\n",
    "    '🤠': ':)',\n",
    "    '🥳': ':)',\n",
    "    '😎': ':)',\n",
    "    '🤓': ':)',\n",
    "    '🧐': ':)',\n",
    "    '😕': ':(',\n",
    "    '😟': ':(',\n",
    "    '🙁': ':(',\n",
    "    '☹': ':(',\n",
    "    '😮': ':O',\n",
    "    '😯': ':O',\n",
    "    '😲': ':O',\n",
    "    '😳': ':(',\n",
    "    '🥺': ':(',\n",
    "    '😦': ':(',\n",
    "    '😧': ':(',\n",
    "    '😨': ':(',\n",
    "    '😰': ':(',\n",
    "    '😥': ':(',\n",
    "    '😢': ':(',\n",
    "    '😭': ':(',\n",
    "    '😱': ':(',\n",
    "    '😖': ':(',\n",
    "    '😣': ':(',\n",
    "    '😞': ':(',\n",
    "    '😓': ':(',\n",
    "    '😩': ':(',\n",
    "    '😫': ':(',\n",
    "    '🥱': 'zzz',\n",
    "    '😤': ':(',\n",
    "    '😡': ':(',\n",
    "    '😠': ':(',\n",
    "    '🤬': ':(',\n",
    "    '😈': ']:->',\n",
    "    '👿': ']:->',\n",
    "    '💀': ':(',\n",
    "    '☠': ':(',\n",
    "    '💋': ':*',\n",
    "    '💔': ':(',\n",
    "    '💤': 'zzz'\n",
    "}\n",
    "\n",
    "class TextNormalizer(object):\n",
    "\n",
    "    def __init__(self, detokenize: bool=True, replace_emoji: bool=True):\n",
    "        self._moses = MosesDetokenizer(lang=\"pl\")\n",
    "        self._detokenize = detokenize\n",
    "        self._replace_emoji = replace_emoji\n",
    "\n",
    "    def process(self, text: str) -> str:\n",
    "        if self._replace_emoji:\n",
    "            text = \"\".join((emoji.get(c, c) for c in text))\n",
    "        if self._detokenize:\n",
    "            text = text.replace(\" em \", \"em \").replace(\" śmy \", \"śmy \").replace(\" m \", \"m \")\n",
    "            tokens: List[str] = text.split()\n",
    "            text = self._moses.detokenize(tokens)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List, Iterable, Callable, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExample(object):\n",
    "\n",
    "    def __init__(self, inputs: Union[str, List], label: str):\n",
    "        self.inputs: List[str] = [inputs] if isinstance(inputs, str) else inputs\n",
    "        self.label: str = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = os.path.join(test_path, 'test_set_only_tags.txt')\n",
    "tags_path = os.path.join(test_path, 'test_set_only_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(self) -> TextNormalizer:\n",
    "        return TextNormalizer(detokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "labels = []\n",
    "with open(text_path, \"r\", encoding=\"utf-8\") as text_file, open(tags_path, \"r\", encoding=\"utf-8\") as tags_file:\n",
    "    text_lines = text_file.readlines()\n",
    "    tags_lines = tags_file.readlines()\n",
    "    assert len(text_lines) == len(tags_lines)\n",
    "    for idx in range(len(text_lines)):\n",
    "        text = normalizer.process(text_lines[idx].strip())\n",
    "        text = text.replace(\"@anonymized_account\", \"@ użytkownik\")\n",
    "        label = tags_lines[idx].strip()\n",
    "        #yield DataExample(text, label)\n",
    "        texts.append(text)\n",
    "        labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchnlp.word_to_vector.FastText(language='en', aligned=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.datasets import imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.word_to_vector import FastText\n",
    "vectors = FastText('pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors['playful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = FastText('pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for sentence in X_test:\n",
    "#     input = tokenizer.encode(sentence)\n",
    "#     output = model(torch.tensor([input.ids]))[0]\n",
    "\n",
    "# #X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
